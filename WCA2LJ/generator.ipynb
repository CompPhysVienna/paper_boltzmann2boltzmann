{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systems.LJ import lennard_jones\n",
    "from systems.dynamic_prior import dynamic_prior\n",
    "\n",
    "from samplers.metropolis_MC import metropolis_monte_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "dimensions = 2\n",
    "n_particles = 32\n",
    "cutin = 0.8\n",
    "\n",
    "T_source = 2\n",
    "beta_source = 1/T_source\n",
    "box_length_source = 6.6\n",
    "rho_source = n_particles/(box_length_source)**(dimensions)\n",
    "WCA = lennard_jones(n_particles=n_particles, dimensions=dimensions, rho=rho_source, device=device, cutin=cutin, cutoff=\"wca\")\n",
    "box_length_pr = WCA.box_length\n",
    "\n",
    "T_target = 1\n",
    "beta_target = 1/T_target\n",
    "box_length_target = 6.6\n",
    "rho_target = n_particles/(box_length_target)**(dimensions)\n",
    "# rho_target = 0.70408163\n",
    "# T_target = 0.60816327\n",
    "# beta_target = 1/T_target\n",
    "scale = (rho_source/rho_target)**(1/dimensions)\n",
    "LJ = lennard_jones(n_particles=n_particles, dimensions=dimensions, rho=rho_target, device=device, cutin=cutin)\n",
    "box_length_sys = LJ.box_length\n",
    "# box_length_target = box_length_sys[0].item()\n",
    "print(f\"rho_source = {rho_source}, T_source = {T_source}\")\n",
    "print(f\"rho_target = {rho_target}, T_target = {T_target}\")\n",
    "print(f\"s = {scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = f\"NVT_N{n_particles:03d}_WCA2LJ_rho_{rho_source:.2g}_T{T_source:.2g}_to_rho_{rho_target:.2g}_T{T_target:.2g}_main\"\n",
    "output_dir = os.path.join(\"./output\", run_id)\n",
    "assert os.path.exists(output_dir), \"Folder with training parameters not found!\"\n",
    "gendir = os.path.join(output_dir, \"generated_confs\")\n",
    "if not os.path.exists(gendir):\n",
    "    os.makedirs(gendir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_pr = metropolis_monte_carlo(system=WCA, step_size=0.2, n_equilibration=5000, n_cycles=1000, transform=True)\n",
    "MCMC_sy = metropolis_monte_carlo(system=LJ, step_size=0.2, n_equilibration=5000, n_cycles=1000, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wca_train_filepath = f\"./data/N{WCA.n_particles:03d}/{WCA.name}/rho_{rho_source:.02g}_T_{T_source:.02g}_train.pt\"\n",
    "wca_sample_filepath = f\"./data/N{WCA.n_particles:03d}/{WCA.name}/rho_{rho_source:.02g}_T_{T_source:.02g}_sample.pt\"\n",
    "\n",
    "print()\n",
    "print(\"Loading WCA Training Datasets\")\n",
    "wca_train = torch.load(wca_train_filepath, map_location=device)\n",
    "print(f\"WCA Train Dataset: {wca_train_filepath}\")\n",
    "wca_sample = torch.load(wca_sample_filepath, map_location=device)\n",
    "print(f\"WCA Sample Dataset: {wca_sample_filepath}\")\n",
    "\n",
    "lj_train_filepath = f\"./data/N{LJ.n_particles:03d}/{LJ.name}/rho_{rho_target:.02g}_T_{T_target:.02g}_train.pt\"\n",
    "lj_sample_filepath = f\"./data/N{LJ.n_particles:03d}/{LJ.name}/rho_{rho_target:.02g}_T_{T_target:.02g}_sample.pt\"\n",
    "\n",
    "print()\n",
    "print(\"Loading LJ Training Datasets\")\n",
    "lj_train = torch.load(lj_train_filepath, map_location=device)\n",
    "print(f\"LJ Train Dataset: {lj_train_filepath}\")\n",
    "lj_sample = torch.load(lj_sample_filepath, map_location=device)\n",
    "print(f\"LJ Sample Dataset: {lj_sample_filepath}\")\n",
    "\n",
    "wca_train_cpu = wca_train.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "wca_sample_cpu = wca_sample.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "lj_train_cpu = lj_train.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "lj_sample_cpu = lj_sample.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "\n",
    "wca_energy_train_cpu = WCA.energy(wca_train).squeeze().cpu().numpy()\n",
    "lj_energy_train_cpu = LJ.energy(lj_train).squeeze().cpu().numpy()\n",
    "wca_energy_sample_cpu = WCA.energy(wca_sample).squeeze().cpu().numpy()\n",
    "lj_energy_sample_cpu = LJ.energy(lj_sample).squeeze().cpu().numpy()\n",
    "\n",
    "print()\n",
    "print(f\"Prior train size: {wca_train.shape[0]}\")\n",
    "print(f\"Prior sample size: {wca_sample.shape[0]}\")\n",
    "print(f\"Posterior train size: {lj_train.shape[0]}\")\n",
    "print(f\"Posterior sample size: {lj_sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "ax.scatter(wca_train_cpu[::50,:,0], wca_train_cpu[::50,:,1], alpha=0.005)\n",
    "ax.scatter(lj_train_cpu[::50,:,0], lj_train_cpu[::50,:,1], alpha=0.005)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  7.5 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "ax.hist(wca_energy_train_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference WCA data\")\n",
    "ax.hist(wca_energy_sample_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference WCA data\")\n",
    "ax.hist(lj_energy_train_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference LJ data\")\n",
    "ax.hist(lj_energy_sample_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference LJ data\")\n",
    "# ax.hist(LJ.energy(wca_train[::10]).cpu().numpy(), bins=40, density=True, alpha=0.5, label=\"Identity WCA to LJ\")\n",
    "# ax.hist(LJ.energy(wca_sample[::10]).cpu().numpy(), bins=40, density=True, label=\"Identity WCA to LJ\")\n",
    "# ax.hist(WCA.energy(lj_train[::10]).cpu().numpy(), bins=40, density=True, alpha=0.5, label=\"Identity LJ to WCA\")\n",
    "# ax.hist(WCA.energy(lj_sample[::10]).cpu().numpy(), bins=40, density=True, label=\"Identity LJ to WCA\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.observables import rdf\n",
    "\n",
    "n_bins = 100\n",
    "cutoff_pr = box_length_source/2\n",
    "cutoff_sys = box_length_target/2\n",
    "RDF_r, RDF_wca_train = rdf(wca_train, n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, cutoff=cutoff_pr, n_bins=n_bins, batch_size=None)\n",
    "RDF_r, RDF_lj_train = rdf(lj_train, n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, cutoff=cutoff_sys, n_bins=n_bins, batch_size=None)\n",
    "RDF_r, RDF_wca_sample = rdf(wca_sample, n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, cutoff=cutoff_pr, n_bins=n_bins, batch_size=None)\n",
    "RDF_r, RDF_lj_sample = rdf(lj_sample, n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, cutoff=cutoff_sys, n_bins=n_bins, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  7.5 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "plt.plot(RDF_r, RDF_wca_train, label=r\"WCA train\")\n",
    "plt.plot(RDF_r, RDF_wca_sample, label=r\"WCA sample\")\n",
    "plt.plot(RDF_r, RDF_lj_train, label=r\"LJ train\")\n",
    "plt.plot(RDF_r, RDF_lj_sample, label=r\"LJ sample\")\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCA = dynamic_prior(n_cached=90000, test_fraction=0.1, system=WCA, sampler=MCMC_pr, init_confs=wca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizing_flow.equivariant_transformer import RQS_coupling_block\n",
    "from normalizing_flow.circular_shift import circular_shift\n",
    "\n",
    "n_bins = 16\n",
    "\n",
    "block_list = [\n",
    "    \n",
    "    # Block 1\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 2\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 3\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 4\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 5\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 6\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 7\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 8\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 9\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 10\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 11\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 12\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 13\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 14\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 15\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 16\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformations.normalization import normalize_box\n",
    "from transformations.remove_origin import remove_origin\n",
    "\n",
    "norm_box_pr = normalize_box(n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, device=device)\n",
    "norm_box_sys = normalize_box(n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, device=device)\n",
    "\n",
    "rm_origin = remove_origin(n_particles=n_particles, dimensions=dimensions, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizing_flow.flow_assembler import flow_assembler\n",
    "\n",
    "flow = flow_assembler(prior = WCA, posterior = LJ, device=device, \n",
    "                        blocks = block_list,\n",
    "                        prior_sided_transformation_layers = [norm_box_pr, rm_origin], \n",
    "                        post_sided_transformation_layers = [norm_box_sys, rm_origin]\n",
    "                        ).to(device)\n",
    "\n",
    "print(f\"Flow parameters: {sum(p.numel() for p in flow.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_parameters_filepath = os.path.join(output_dir, \"flow_parameters.pt\")\n",
    "print(f\"Loading network parameters from {flow_parameters_filepath}\")\n",
    "flow.load_state_dict(torch.load(flow_parameters_filepath))\n",
    "metrics_filepath = os.path.join(output_dir, \"train_log.txt\")\n",
    "print(f\"Loading metrics from {metrics_filepath}\")\n",
    "metrics = np.loadtxt(metrics_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_x = 50 if metrics.shape[1] <= 8 else 75\n",
    "fig_size = (length_x * 0.393701, 10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 3 if metrics.shape[1] <= 8 else 5, figsize = fig_size, dpi = 600)\n",
    "\n",
    "if metrics[:,1].mean() != 0:\n",
    "    ax[0].plot(metrics[:,0], metrics[:,1], label=\"train\", color=\"C0\")\n",
    "ax[0].plot(metrics[:,0], metrics[:,3], label=\"eval\", color=\"C1\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "ax[0].set_ylabel(\"NLL loss\")\n",
    "\n",
    "if metrics[:,2].mean() != 0:\n",
    "    ax[1].plot(metrics[:,0], metrics[:,2], label=\"train\", color=\"C0\")\n",
    "ax[1].plot(metrics[:,0], metrics[:,5], label=\"eval\", color=\"C1\")\n",
    "ax[1].set_xlabel(\"epochs\")\n",
    "ax[1].set_ylabel(\"KLD loss\")\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "ax[2].plot(metrics[:,0], metrics[:,4], label=r\"$x\\to z$\", color=\"C2\")\n",
    "ax[2].plot(metrics[:,0], metrics[:,6], label=r\"$z\\to x$\", color=\"C3\")\n",
    "ax[2].set_xlabel(\"epochs\")\n",
    "ax[2].set_ylabel(\"ress\")\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_ylim(None,1)\n",
    "ax[2].legend(frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from tools.util import ress\n",
    "\n",
    "load_replicas_zx = False\n",
    "\n",
    "N_replicas = 10\n",
    "NN_generated_samples = 500000\n",
    "NN_replica_size = NN_generated_samples//N_replicas\n",
    "\n",
    "print(f\"NN_generated_samples = {NN_generated_samples}\")\n",
    "print(f\"N_replicas = {N_replicas}\")\n",
    "print(f\"NN_replica_size = {NN_replica_size}\")\n",
    "print()\n",
    "\n",
    "if load_replicas_zx:\n",
    "    print(\"Loading source configurations from file\")\n",
    "else:\n",
    "    print(\"Generating source configurations\")\n",
    "    print(\"Equilibrating sampler\")\n",
    "    z, _, _ = MCMC_pr.sample_space(NN_replica_size, 0.2*beta_source)\n",
    "    MCMC_pr.equilibrated = False\n",
    "    z, _, _ = MCMC_pr.sample_space(NN_replica_size, beta_source)\n",
    "\n",
    "print(\"Transforming source to target... \", flush=True, end = \"\")\n",
    "with open(os.path.join(gendir, \"time_zx.out\"), \"w\") as fout_t:\n",
    "    fout_t.write(\"# replica\\tsample\\ttransform\\n\")\n",
    "    with open(os.path.join(gendir, \"ress_zx.out\"), \"w\") as fout:\n",
    "        fout.write(\"# replica\\tid_ress_zx\\tress_zx\\n\")\n",
    "        for r in tqdm(range(N_replicas), ncols=100, desc=\"\\tProgress\"):\n",
    "            # PyTorch does not need the gradient for the transformation \n",
    "            with torch.no_grad():\n",
    "\n",
    "                flow.eval()\n",
    "\n",
    "                # load replicas previously generated\n",
    "                sample_stime = datetime.datetime.now()\n",
    "                if load_replicas_zx:\n",
    "                    z_generated_filepath = os.path.join(gendir, f\"z_{r:04d}.pt\")\n",
    "                    z = torch.load(z_generated_filepath, map_location=device)\n",
    "                else: # generate replicas\n",
    "                    z, _, _ = MCMC_pr.sample_space(NN_replica_size, beta_source)\n",
    "                    z_generated_filepath = os.path.join(gendir, f\"z_{r:04d}.pt\")\n",
    "                    torch.save(z, z_generated_filepath)\n",
    "\n",
    "                # Transforming from latent to target via the Normalizing Flow\n",
    "                sample_etime = datetime.datetime.now()\n",
    "                Tz, logJ_zx = flow.F_zx(z)\n",
    "                transf_etime = datetime.datetime.now()\n",
    "\n",
    "                # Compute energy of identity transformations\n",
    "                id_energy_x = flow.posterior.energy(z*scale)\n",
    "\n",
    "                # Computing weights\n",
    "                id_log_prob_zx = -beta_target*id_energy_x\n",
    "                id_log_prob_z = -beta_source*flow.prior.energy(z)        \n",
    "                id_log_w = (id_log_prob_zx - id_log_prob_z).squeeze(-1)\n",
    "                id_ress_zx = ress(id_log_w)\n",
    "\n",
    "                # Compute energy of transformed configurations\n",
    "                WCA2LJ_energy_transformed = (LJ.energy(Tz)).cpu().numpy()\n",
    "\n",
    "                # Computing weights\n",
    "                log_prob_zx = -beta_target*flow.posterior.energy(Tz)\n",
    "                log_prob_z = -beta_source*flow.prior.energy(z)        \n",
    "                log_w_zx = (log_prob_zx - log_prob_z + logJ_zx).squeeze(-1)\n",
    "                ress_zx = ress(log_w_zx)\n",
    "\n",
    "                # Resampling to obtain unbiased target distribution\n",
    "                Tz_cpu = Tz.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "                w_zx = torch.exp(log_w_zx - torch.max(log_w_zx)).cpu().numpy()\n",
    "                N = Tz_cpu.shape[0]\n",
    "                indx = np.random.choice(np.arange(0, N), replace=True, size = N, p = w_zx/np.sum(w_zx))\n",
    "                Tz_resampled = torch.from_numpy(Tz_cpu[indx].reshape(-1, n_particles*dimensions)).to(device)\n",
    "\n",
    "                fout_t.write(f\"{r}\\t{sample_etime-sample_stime}\\t{transf_etime-sample_etime}\\n\")\n",
    "                fout.write(f\"{r}\\t{id_ress_zx}\\t{ress_zx}\\n\")\n",
    "\n",
    "            Tz_generated_filepath = os.path.join(gendir, f\"Tz_{r:04d}.pt\")\n",
    "            torch.save(Tz, Tz_generated_filepath)\n",
    "            log_w_zx_generated_filepath = os.path.join(gendir, f\"log_w_zx_{r:04d}.pt\")\n",
    "            torch.save(log_w_zx, log_w_zx_generated_filepath)\n",
    "            Tz_resampled_generated_filepath = os.path.join(gendir, f\"Tz_resampled_{r:04d}.pt\")\n",
    "            torch.save(Tz_resampled, Tz_resampled_generated_filepath)\n",
    "\n",
    "print(\"Done\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "load_replicas_xz = False\n",
    "\n",
    "N_replicas = 10\n",
    "NN_generated_samples = 500000\n",
    "NN_replica_size = NN_generated_samples//N_replicas\n",
    "\n",
    "print(f\"NN_generated_samples = {NN_generated_samples}\")\n",
    "print(f\"N_replicas = {N_replicas}\")\n",
    "print(f\"NN_replica_size = {NN_replica_size}\")\n",
    "print()\n",
    "\n",
    "if load_replicas_xz:\n",
    "    print(\"Loading target configurations from file\")\n",
    "else:\n",
    "    print(\"Generating target configurations\")\n",
    "    print(\"Equilibrating sampler\")\n",
    "    x, _, _ = MCMC_sy.sample_space(NN_replica_size, 0.2*beta_target)\n",
    "    MCMC_sy.equilibrated = False\n",
    "    x, _, _ = MCMC_sy.sample_space(NN_replica_size, beta_target)\n",
    "\n",
    "print(\"Transforming target to source... \", flush=True, end = \"\")\n",
    "with open(os.path.join(gendir, \"ress_xz.out\"), \"w\") as fout:\n",
    "    fout.write(\"# replica\\tid_ress_xz\\tress_xz\\n\")\n",
    "    for r in tqdm(range(N_replicas), ncols=100, desc=\"\\tProgress\"):\n",
    "        # PyTorch does not need the gradient for the transformation \n",
    "        with torch.no_grad():\n",
    "\n",
    "            flow.eval()\n",
    "\n",
    "            # load replicas previously generated\n",
    "            if load_replicas_xz:\n",
    "                x_generated_filepath = os.path.join(gendir, f\"x_{r:04d}.pt\")\n",
    "                x = torch.load(x_generated_filepath, map_location=device)\n",
    "            else: # generate replicas\n",
    "                x, _, _ = MCMC_sy.sample_space(NN_replica_size, beta_target)\n",
    "                x_generated_filepath = os.path.join(gendir, f\"x_{r:04d}.pt\")\n",
    "                torch.save(x, x_generated_filepath)\n",
    "\n",
    "            # Transforming from latent to target via the Normalizing Flow\n",
    "            Tinvx, logJ_xz = flow.F_xz(x)\n",
    "\n",
    "            # Compute energy of identity transformations\n",
    "            id_energy_z = flow.prior.energy(x/scale)\n",
    "\n",
    "            # Computing weights\n",
    "            id_log_prob_xz = -beta_source*id_energy_z\n",
    "            id_log_prob_x = -beta_target*flow.posterior.energy(x)        \n",
    "            id_log_w = (id_log_prob_xz - id_log_prob_x).squeeze(-1)\n",
    "            id_ress_xz = ress(id_log_w)\n",
    "\n",
    "            # Compute energy of transformed configurations\n",
    "            LJ2WCA_energy_transformed = (WCA.energy(Tinvx)).cpu().numpy()\n",
    "\n",
    "            # Computing weights\n",
    "            log_prob_xz = -beta_source*flow.prior.energy(Tinvx)\n",
    "            log_prob_x = -beta_target*flow.posterior.energy(x)        \n",
    "            log_w_xz = (log_prob_xz - log_prob_x + logJ_xz).squeeze(-1)\n",
    "            ress_xz = ress(log_w_xz)\n",
    "\n",
    "            fout.write(f\"{r}\\t{id_ress_xz}\\t{ress_xz}\\n\")\n",
    "\n",
    "            # Resampling to obtain unbiased target distribution\n",
    "            Tinvx_cpu = Tinvx.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "            w_xz = torch.exp(log_w_xz - torch.max(log_w_xz)).cpu().numpy()\n",
    "            N = Tinvx_cpu.shape[0]\n",
    "            indx = np.random.choice(np.arange(0, N), replace=True, size = N, p = w_xz/np.sum(w_xz))\n",
    "            Tinvx_resampled = torch.from_numpy(Tinvx_cpu[indx].reshape(-1, n_particles*dimensions)).to(device)\n",
    "        \n",
    "        Tinvx_generated_filepath = os.path.join(gendir, f\"Tinvx_{r:04d}.pt\")\n",
    "        torch.save(Tinvx, Tinvx_generated_filepath)\n",
    "        log_w_xz_generated_filepath = os.path.join(gendir, f\"log_w_xz_{r:04d}.pt\")\n",
    "        torch.save(log_w_xz, log_w_xz_generated_filepath)\n",
    "        Tinvx_resampled_generated_filepath = os.path.join(gendir, f\"Tinvx_resampled_{r:04d}.pt\")\n",
    "        torch.save(Tinvx_resampled, Tinvx_resampled_generated_filepath)\n",
    "\n",
    "print(\"Done\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
