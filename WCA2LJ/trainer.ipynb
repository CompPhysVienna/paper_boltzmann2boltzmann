{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from systems.LJ import lennard_jones\n",
    "from systems.dynamic_prior import dynamic_prior\n",
    "\n",
    "from samplers.metropolis_MC import metropolis_monte_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "dimensions = 2\n",
    "n_particles = 32\n",
    "cutin = 0.8\n",
    "\n",
    "T_source = 2\n",
    "beta_source = 1/T_source\n",
    "box_length_source = 6.6\n",
    "rho_source = n_particles/(box_length_source)**(dimensions)\n",
    "WCA = lennard_jones(n_particles=n_particles, dimensions=dimensions, rho=rho_source, device=device, cutin=cutin, cutoff=\"wca\")\n",
    "box_length_pr = WCA.box_length\n",
    "\n",
    "T_target = 1\n",
    "beta_target = 1/T_target\n",
    "box_length_target = 6.6 \n",
    "rho_target = n_particles/(box_length_target)**(dimensions)\n",
    "# rho_target = 0.70408163\n",
    "# T_target = 0.60816327\n",
    "# beta_target = 1/T_target\n",
    "LJ = lennard_jones(n_particles=n_particles, dimensions=dimensions, rho=rho_target, device=device, cutin=cutin)\n",
    "box_length_sys = LJ.box_length\n",
    "# box_length_target = box_length_sys[0].item()\n",
    "scale = (rho_source/rho_target)**(1/dimensions)\n",
    "\n",
    "print(f\"rho_source = {rho_source}, T_source = {T_source}\")\n",
    "print(f\"rho_target = {rho_target}, T_target = {T_target}\")\n",
    "print(f\"s = {scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.util import generate_output_directory\n",
    "\n",
    "run_id = f\"NVT_N{n_particles:03d}_WCA2LJ_rho_{rho_source:.2g}_T{T_source:.2g}_to_rho_{rho_target:.2g}_T{T_target:.2g}_main\"\n",
    "output_dir = generate_output_directory(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "MCMC_pr = metropolis_monte_carlo(system=WCA, step_size=0.2, n_equilibration=5000, n_cycles=1000, transform=True)\n",
    "MCMC_sy = metropolis_monte_carlo(system=LJ, step_size=0.2, n_equilibration=5000, n_cycles=1000, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "load_data_pr = True\n",
    "load_data_sy = True\n",
    "\n",
    "wca_train_filepath = f\"./data/N{WCA.n_particles:03d}/{WCA.name}/rho_{rho_source:.02g}_T_{T_source:.02g}_train.pt\"\n",
    "wca_sample_filepath = f\"./data/N{WCA.n_particles:03d}/{WCA.name}/rho_{rho_source:.02g}_T_{T_source:.02g}_sample.pt\"\n",
    "\n",
    "n_samples_pr = 100000\n",
    "n_samples_sy = 100000\n",
    "\n",
    "if load_data_pr:\n",
    "    print()\n",
    "    print(\"Loading WCA Training Datasets\")\n",
    "    wca_train = torch.load(wca_train_filepath, map_location=device)\n",
    "    print(f\"WCA Train Dataset: {wca_train_filepath}\")\n",
    "    wca_sample = torch.load(wca_sample_filepath, map_location=device)\n",
    "    print(f\"WCA Sample Dataset: {wca_sample_filepath}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"Generating WCA Training Datasets\")\n",
    "    wca_train, _, acc = MCMC_pr.sample_space(n_samples_pr, 0.2*beta_source)\n",
    "    MCMC_pr.equilibrated = False\n",
    "    wca_train, _, acc = MCMC_pr.sample_space(n_samples_pr, beta_source)\n",
    "    print(f\"WCA Train Dataset: acc = {acc.item()}\")\n",
    "    wca_sample, _, acc = MCMC_pr.sample_space(n_samples_pr, beta_source)\n",
    "    print(f\"WCA Sample Dataset: acc = {acc.item()}\")\n",
    "    \n",
    "    torch.save(wca_train, wca_train_filepath)\n",
    "    print(f\"WCA Train Dataset: {wca_train_filepath}\")\n",
    "    torch.save(wca_sample, wca_sample_filepath)\n",
    "    print(f\"WCA Sample Dataset: {wca_sample_filepath}\")\n",
    "\n",
    "\n",
    "lj_train_filepath = f\"./data/N{LJ.n_particles:03d}/{LJ.name}/rho_{rho_target:.02g}_T_{T_target:.02g}_train.pt\"\n",
    "lj_sample_filepath = f\"./data/N{LJ.n_particles:03d}/{LJ.name}/rho_{rho_target:.02g}_T_{T_target:.02g}_sample.pt\"\n",
    "\n",
    "if load_data_sy:\n",
    "    print()\n",
    "    print(\"Loading LJ Training Datasets\")\n",
    "    lj_train = torch.load(lj_train_filepath, map_location=device)\n",
    "    print(f\"LJ Train Dataset: {lj_train_filepath}\")\n",
    "    lj_sample = torch.load(lj_sample_filepath, map_location=device)\n",
    "    print(f\"LJ Sample Dataset: {lj_sample_filepath}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"Generating LJ Training Datasets\")\n",
    "    lj_train, _, acc = MCMC_sy.sample_space(n_samples_sy, 0.2*beta_target)\n",
    "    MCMC_sy.equilibrated = False\n",
    "    lj_train, _, acc = MCMC_sy.sample_space(n_samples_sy, beta_target)\n",
    "    print(f\"LJ Train Dataset: acc = {acc.item()}\")\n",
    "    lj_sample, _, acc = MCMC_sy.sample_space(n_samples_sy, beta_target)\n",
    "    print(f\"LJ Sample Dataset: acc = {acc.item()}\")\n",
    "    \n",
    "    torch.save(lj_train, lj_train_filepath)\n",
    "    print(f\"LJ Train Dataset: {lj_train_filepath}\")\n",
    "    torch.save(lj_sample, lj_sample_filepath)\n",
    "    print(f\"LJ Sample Dataset: {lj_sample_filepath}\")\n",
    "\n",
    "wca_train_cpu = wca_train.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "wca_sample_cpu = wca_sample.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "lj_train_cpu = lj_train.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "lj_sample_cpu = lj_sample.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "\n",
    "wca_energy_train_cpu = WCA.energy(wca_train).squeeze().cpu().numpy()\n",
    "lj_energy_train_cpu = LJ.energy(lj_train).squeeze().cpu().numpy()\n",
    "wca_energy_sample_cpu = WCA.energy(wca_sample).squeeze().cpu().numpy()\n",
    "lj_energy_sample_cpu = LJ.energy(lj_sample).squeeze().cpu().numpy()\n",
    "\n",
    "print()\n",
    "print(f\"Prior train size: {wca_train.shape[0]}\")\n",
    "print(f\"Prior sample size: {wca_sample.shape[0]}\")\n",
    "print(f\"Posterior train size: {lj_train.shape[0]}\")\n",
    "print(f\"Posterior sample size: {lj_sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "ax.scatter(wca_train_cpu[::50,:,0], wca_train_cpu[::50,:,1], alpha=0.005, label=\"WCA\")\n",
    "ax.scatter(lj_train_cpu[::50,:,0], lj_train_cpu[::50,:,1], alpha=0.005, label=\"LJ\")\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"configurations.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  7.5 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "ax.hist(wca_energy_train_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference WCA data\")\n",
    "ax.hist(wca_energy_sample_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference WCA data\")\n",
    "ax.hist(lj_energy_train_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference LJ data\")\n",
    "ax.hist(lj_energy_sample_cpu[::10], bins=40, density=True, alpha=0.5, label=\"Reference LJ data\")\n",
    "# ax.hist(LJ.energy(wca_train[::10]).cpu().numpy(), bins=40, density=True, alpha=0.5, label=\"Identity WCA to LJ\")\n",
    "# ax.hist(LJ.energy(wca_sample[::10]).cpu().numpy(), bins=40, density=True, label=\"Identity WCA to LJ\")\n",
    "# ax.hist(WCA.energy(lj_train[::10]).cpu().numpy(), bins=40, density=True, alpha=0.5, label=\"Identity LJ to WCA\")\n",
    "# ax.hist(WCA.energy(lj_sample[::10]).cpu().numpy(), bins=40, density=True, label=\"Identity LJ to WCA\")\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"energies.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_length_target=box_length_sys[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.observables import rdf\n",
    "\n",
    "n_bins = 100\n",
    "cutoff_pr = box_length_source/2\n",
    "cutoff_sys = box_length_target/2\n",
    "RDF_r, RDF_wca_train = rdf(wca_train, n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, cutoff=cutoff_pr, n_bins=n_bins)\n",
    "RDF_r, RDF_lj_train = rdf(lj_train, n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, cutoff=cutoff_sys, n_bins=n_bins)\n",
    "RDF_r, RDF_wca_sample = rdf(wca_sample, n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, cutoff=cutoff_pr, n_bins=n_bins)\n",
    "RDF_r, RDF_lj_sample = rdf(lj_sample, n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, cutoff=cutoff_sys, n_bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (10 * 0.393701,  7.5 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 1, figsize = fig_size, dpi = 100)\n",
    "\n",
    "plt.plot(RDF_r, RDF_wca_train, label=r\"WCA train\")\n",
    "plt.plot(RDF_r, RDF_wca_sample, label=r\"WCA sample\")\n",
    "plt.plot(RDF_r, RDF_lj_train, label=r\"LJ train\")\n",
    "plt.plot(RDF_r, RDF_lj_sample, label=r\"LJ sample\")\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(os.path.join(output_dir, \"rdfs.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "WCA = dynamic_prior(n_cached=90000, test_fraction=0.1, system=WCA, sampler=MCMC_pr, init_confs=wca_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-sided Circular Spline Flow Equivariant Transformer Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_blocks = 1\n",
    "n_bins = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tools.util import get_targets\n",
    "\n",
    "targets = get_targets(dimensions, n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of NF block list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from normalizing_flow.equivariant_transformer import RQS_coupling_block\n",
    "from normalizing_flow.circular_shift import circular_shift\n",
    "\n",
    "block_list = [\n",
    "    \n",
    "    # Block 1\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 2\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 3\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 4\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 5\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 6\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # Block 7\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # Block 8\n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    circular_shift(n_particles-1, dimensions, device),\n",
    "    RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 9\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 10\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 11\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 12\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 13\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 14\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "\n",
    "    # # Block 15\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # # Block 16\n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    \n",
    "    # circular_shift(n_particles-1, dimensions, device),\n",
    "    # RQS_coupling_block((1,), n_particles-1, dimensions, device, n_bins),\n",
    "    # RQS_coupling_block((0,), n_particles-1, dimensions, device, n_bins),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Layers definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformations.normalization import normalize_box\n",
    "from transformations.remove_origin import remove_origin\n",
    "\n",
    "norm_box_pr = normalize_box(n_particles=n_particles, dimensions=dimensions, box_length=box_length_pr, device=device)\n",
    "norm_box_sys = normalize_box(n_particles=n_particles, dimensions=dimensions, box_length=box_length_sys, device=device)\n",
    "\n",
    "rm_origin = remove_origin(n_particles=n_particles, dimensions=dimensions, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from normalizing_flow.flow_assembler import flow_assembler\n",
    "\n",
    "flow = flow_assembler(prior = WCA, posterior = LJ, device=device, \n",
    "                        blocks = block_list,\n",
    "                        prior_sided_transformation_layers = [norm_box_pr, rm_origin], \n",
    "                        post_sided_transformation_layers = [norm_box_sys, rm_origin]\n",
    "                        ).to(device)\n",
    "\n",
    "print(f\"Flow parameters: {sum(p.numel() for p in flow.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from normalizing_flow.dataset import PBCDataset\n",
    "\n",
    "train_dataset = PBCDataset(flow, data_tensor=lj_train, test_fraction=0.1, beta_source=beta_source, beta_target=beta_target, shuffle_data=False, transform=True, augment=True, energy_labels=LJ.energy(lj_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 512\n",
    "n_dump = 1\n",
    "n_save = 5\n",
    "\n",
    "steps_per_epoch = len(train_dataset)//batch_size\n",
    "print(f\"Total number of optimization steps: {n_epochs*steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xz = 1\n",
    "w_zx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from normalizing_flow.network_trainer import Trainer \n",
    "\n",
    "flow_trainer = Trainer(flow)\n",
    "\n",
    "optimizer = None\n",
    "optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad], lr=1e-4)\n",
    "\n",
    "scheduler = None\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5*steps_per_epoch, 7*steps_per_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train_start_time = datetime.datetime.now()\n",
    "print(\"Training the network:\\n\")\n",
    "metrics = flow_trainer.training_routine(train_dataset, beta_source=beta_source, beta_target=beta_target, \n",
    "                                        w_xz=w_xz, w_zx=w_zx, batch_size=batch_size,\n",
    "                                        n_epochs=n_epochs, n_dump=n_dump, n_save=n_save, save_dir=output_dir, \n",
    "                                        optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (50 * 0.393701, 10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, 3, figsize = fig_size, dpi = 600)\n",
    "\n",
    "if w_xz > 0:\n",
    "    ax[0].plot(metrics[:,0], metrics[:,1], label=\"train\", color=\"C0\")\n",
    "ax[0].plot(metrics[:,0], metrics[:,3], label=\"eval\", color=\"C1\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "ax[0].set_ylabel(\"NLL loss\")\n",
    "\n",
    "if w_zx > 0:\n",
    "    ax[1].plot(metrics[:,0], metrics[:,2], label=\"training\", color=\"C0\")\n",
    "ax[1].plot(metrics[:,0], metrics[:,5], label=\"validation\", color=\"C1\")\n",
    "ax[1].set_xlabel(\"epochs\")\n",
    "ax[1].set_ylabel(\"KLD loss\")\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "ax[2].plot(metrics[:,0], metrics[:,4], label=r\"$\\text{A}\\to \\text{B}$\", color=\"C2\")\n",
    "ax[2].plot(metrics[:,0], metrics[:,6], label=r\"$\\text{B}\\to \\text{A}$\", color=\"C3\")\n",
    "ax[2].set_xlabel(\"epochs\")\n",
    "ax[2].set_ylabel(\"RESS\")\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_ylim(None,1)\n",
    "ax[2].legend(frameon=False)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"metrics.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"run_details.out\"), \"w+\") as f:\n",
    "    \n",
    "    f.write(f\"Run ID: {run_id}\\n\")\n",
    "    f.write(f\"Training started on: {train_start_time}\\n\")\n",
    "    f.write(f\"Training finished on: {datetime.datetime.now()}\\n\\n\")\n",
    "    f.write(f\"z: {flow.prior.name} -> x: {flow.posterior.name}\\n\")\n",
    "    f.write(f\"Tz: {T_source} -> Tx: {T_target}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"source training data: {wca_train_filepath}\\n\")\n",
    "    f.write(f\"target training data: {lj_train_filepath}\\n\")\n",
    "    f.write(f\"source sample data: {wca_sample_filepath}\\n\")\n",
    "    f.write(f\"target sample data: {lj_sample_filepath}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"elements in source training data: {wca_train.shape[0]}\\n\")\n",
    "    f.write(f\"elements in target training data: {lj_train.shape[0]}\\n\")\n",
    "    f.write(f\"elements in source sample data: {wca_sample.shape[0]}\\n\")\n",
    "    f.write(f\"elements in target sample data: {lj_sample.shape[0]}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"batch size: {batch_size}\\n\")\n",
    "    if w_xz > 0:\n",
    "        f.write(f\"Training x->z: w_xz = {w_xz}\\n\")\n",
    "    if w_zx > 0:\n",
    "        f.write(f\"Training z->x: w_zx = {w_zx}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Flow architecture:\\n\")\n",
    "    f.write(str(flow) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate from the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "WCA2LJ_energy_identity = (LJ.energy(scale*wca_sample)).cpu().numpy()\n",
    "LJ2WCA_energy_identity = (WCA.energy(lj_sample/scale)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tools.util import ress\n",
    "\n",
    "# PyTorch does not need the gradient for the transformation \n",
    "with torch.no_grad():\n",
    "\n",
    "    flow.eval()\n",
    "\n",
    "    # Transforming from latent to target via the Normalizing Flow\n",
    "    z = wca_sample[::10]\n",
    "    x, logJ_zx = flow.F_zx(z)\n",
    "\n",
    "    # Compute energy of transformed configurations\n",
    "    WCA2LJ_energy_transformed = (LJ.energy(x)).cpu().numpy()\n",
    "\n",
    "    # Computing weights\n",
    "    log_prob_zx = -beta_target*flow.posterior.energy(x)\n",
    "    log_prob_z = -beta_source*flow.prior.energy(z)        \n",
    "    log_w = (log_prob_zx - log_prob_z + logJ_zx).squeeze(-1)\n",
    "    ress_zx = ress(log_w)\n",
    "\n",
    "    print(f\"RESS zx = {ress_zx}\")\n",
    "\n",
    "    # Resampling to obtain unbiased target distribution\n",
    "    x_cpu = x.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "    w = torch.exp(log_w - torch.max(log_w)).cpu().numpy()\n",
    "    N = x_cpu.shape[0]\n",
    "    indx = np.random.choice(np.arange(0, N), replace=True, size = N, p = w/np.sum(w))\n",
    "    x_resampled = x_cpu[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    id_x = wca_sample[::10]\n",
    "\n",
    "    # Compute energy of transformed configurations\n",
    "    id_energy_x = flow.posterior.energy(scale*id_x)\n",
    "\n",
    "    # Computing weights\n",
    "    id_log_prob_zx = -beta_target*id_energy_x\n",
    "    id_log_prob_z = -beta_source*flow.prior.energy(z)        \n",
    "    id_log_w = (id_log_prob_zx - id_log_prob_z).squeeze(-1)\n",
    "    id_ress_zx = ress(id_log_w)\n",
    "\n",
    "    print(f\"id RESS zx = {id_ress_zx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (20 * 0.393701, 20 * 0.393701)\n",
    "fig, ax = plt.subplots(2, 2, figsize = fig_size, dpi = 600)\n",
    "\n",
    "ax[0][0].scatter(wca_sample_cpu[::10, :, 0], wca_sample_cpu[::10, :, 1], alpha=0.100, s=0.25)\n",
    "ax[0][0].set_title(r'$x_{\\text{A}} \\sim \\rho_\\text{A}(x_{\\text{A}})$')\n",
    "ax[0][0].set_xlim(-box_length_pr[0].item()*(1+0.1)/2,box_length_pr[0].item()*(1+0.1)/2)\n",
    "ax[0][0].set_ylim(-box_length_pr[1].item()*(1+0.1)/2,box_length_pr[1].item()*(1+0.1)/2)\n",
    "\n",
    "ax[0][1].scatter(x_cpu[:, :, 0], x_cpu[:, :, 1], alpha=0.100, s=0.25)\n",
    "ax[0][1].set_title(r'$x_{\\text{B}} = F(x_{\\text{A}})$')\n",
    "ax[0][1].set_xlim(-box_length_sys[0].item()*(1+0.1)/2,box_length_sys[0].item()*(1+0.1)/2)\n",
    "ax[0][1].set_ylim(-box_length_sys[1].item()*(1+0.1)/2,box_length_sys[1].item()*(1+0.1)/2)\n",
    "    \n",
    "ax[1][0].scatter(x_resampled[:, :, 0], x_resampled[:, :, 1], alpha=0.100, s=0.25)\n",
    "ax[1][0].set_title(r'$x_{\\text{B}} = \\bar{F}(x_{\\text{A}})$')\n",
    "ax[1][0].set_xlim(-box_length_sys[0].item()*(1+0.1)/2,box_length_sys[0].item()*(1+0.1)/2)\n",
    "ax[1][0].set_ylim(-box_length_sys[1].item()*(1+0.1)/2,box_length_sys[1].item()*(1+0.1)/2)\n",
    "\n",
    "ax[1][1].scatter(lj_sample_cpu[::10, :, 0], lj_sample_cpu[::10, :, 1], alpha=0.100, s=0.25)\n",
    "ax[1][1].set_title(r'Reference B')\n",
    "ax[1][1].set_xlim(-box_length_sys[0].item()*(1+0.1)/2,box_length_sys[0].item()*(1+0.1)/2)\n",
    "ax[1][1].set_ylim(-box_length_sys[1].item()*(1+0.1)/2,box_length_sys[1].item()*(1+0.1)/2)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"WCA2LJ_confs.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (15 * 0.393701, 10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, figsize = fig_size, dpi = 600)\n",
    "\n",
    "ax.hist(lj_energy_sample_cpu, bins=100, density=True, alpha=0.5, label=\"Reference\")\n",
    "ax.hist(WCA2LJ_energy_identity, bins=100, density=True, alpha=0.5, label=\"Identity\")\n",
    "ax.hist(WCA2LJ_energy_transformed, bins=100, density=True, alpha=0.5, label=\"Transformed\")\n",
    "ax.set_xlabel(r\"$U(x)$\")\n",
    "ax.set_ylabel(r\"$P(U)$\")\n",
    "ax.set_title(\"Energy of Target System\")\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(os.path.join(output_dir, \"WCA2LJ_ener.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tools.util import ress\n",
    "\n",
    "# PyTorch does not need the gradient for the transformation \n",
    "with torch.no_grad():\n",
    "\n",
    "    flow.eval()\n",
    "\n",
    "    # Transforming from latent to target via the Normalizing Flow\n",
    "    x = lj_sample[::10]\n",
    "    z, logJ_xz = flow.F_xz(x)\n",
    "\n",
    "    # Compute energy of transformed configurations\n",
    "    LJ2WCA_energy_transformed = (WCA.energy(z)).cpu().numpy()\n",
    "\n",
    "    # Computing weights\n",
    "    log_prob_xz = -beta_source*flow.prior.energy(z)\n",
    "    log_prob_x = -beta_target*flow.posterior.energy(x)        \n",
    "    log_w = (log_prob_xz - log_prob_x + logJ_xz).squeeze(-1)\n",
    "    ress_xz = ress(log_w)\n",
    "\n",
    "    print(f\"RESS xz = {ress_xz}\")\n",
    "\n",
    "    # Resampling to obtain unbiased target distribution\n",
    "    z_cpu = z.view(-1, n_particles, dimensions).cpu().numpy()\n",
    "    w = torch.exp(log_w - torch.max(log_w)).cpu().numpy()\n",
    "    N = z_cpu.shape[0]\n",
    "    indx = np.random.choice(np.arange(0, N), replace=True, size = N, p = w/np.sum(w))\n",
    "    z_resampled = z_cpu[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch does not need the gradient for the transformation \n",
    "with torch.no_grad():\n",
    "\n",
    "    flow.eval()\n",
    "\n",
    "    # Transforming from latent to target via the Normalizing Flow\n",
    "    id_z = lj_sample[::10]\n",
    "\n",
    "    # Compute energy of transformed configurations\n",
    "    id_energy_z = flow.prior.energy(id_z/scale)\n",
    "\n",
    "    # Computing weights\n",
    "    id_log_prob_xz = -beta_source*id_energy_z\n",
    "    id_log_prob_x = -beta_target*flow.posterior.energy(x)        \n",
    "    id_log_w = (id_log_prob_xz - id_log_prob_x).squeeze(-1)\n",
    "    id_ress_xz = ress(id_log_w)\n",
    "\n",
    "    print(f\"id RESS xz = {id_ress_xz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (20 * 0.393701, 20 * 0.393701)\n",
    "fig, ax = plt.subplots(2, 2, figsize = fig_size, dpi = 600)\n",
    "\n",
    "ax[0][0].scatter(lj_sample_cpu[::10, :, 0], lj_sample_cpu[::10, :, 1], alpha=0.100, s=0.25)\n",
    "ax[0][0].set_title(r'$x_{\\text{B}} \\sim \\rho_\\text{B}(x_{\\text{B}})$')\n",
    "ax[0][0].set_xlim(-box_length_sys[0].item()*(1+0.1)/2,box_length_sys[0].item()*(1+0.1)/2)\n",
    "ax[0][0].set_ylim(-box_length_sys[1].item()*(1+0.1)/2,box_length_sys[1].item()*(1+0.1)/2)\n",
    "\n",
    "ax[0][1].scatter(z_cpu[:, :, 0], z_cpu[:, :, 1], alpha=0.100, s=0.25)\n",
    "ax[0][1].set_title(r'$x_{\\text{A}} = F^{-1}(x_{\\text{B}})$')\n",
    "ax[0][1].set_xlim(-box_length_pr[0].item()*(1+0.1)/2,box_length_pr[0].item()*(1+0.1)/2)\n",
    "ax[0][1].set_ylim(-box_length_pr[1].item()*(1+0.1)/2,box_length_pr[1].item()*(1+0.1)/2)\n",
    "    \n",
    "ax[1][0].scatter(z_resampled[:, :, 0], z_resampled[:, :, 1], alpha=0.100, s=0.25)\n",
    "ax[1][0].set_title(r'$x_{\\text{A}} = \\bar{F}^{-1}({\\text{B}})$')\n",
    "ax[1][0].set_xlim(-box_length_pr[0].item()*(1+0.1)/2,box_length_pr[0].item()*(1+0.1)/2)\n",
    "ax[1][0].set_ylim(-box_length_pr[1].item()*(1+0.1)/2,box_length_pr[1].item()*(1+0.1)/2)\n",
    "\n",
    "ax[1][1].scatter(wca_sample_cpu[::10, :, 0], wca_sample_cpu[::10, :, 1], alpha=0.100, s=0.25)\n",
    "ax[1][1].set_title(r'Reference A')\n",
    "ax[1][1].set_xlim(-box_length_pr[0].item()*(1+0.1)/2,box_length_pr[0].item()*(1+0.1)/2)\n",
    "ax[1][1].set_ylim(-box_length_pr[0].item()*(1+0.1)/2,box_length_pr[0].item()*(1+0.1)/2)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"LJ2WCA_confs.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig_size = (15 * 0.393701, 10 * 0.393701)\n",
    "fig, ax = plt.subplots(1, figsize = fig_size, dpi = 600)\n",
    "\n",
    "ax.hist(wca_energy_sample_cpu, bins=100, density=True, alpha=0.5, label=\"Reference\")\n",
    "ax.hist(LJ2WCA_energy_identity, bins=100, density=True, alpha=0.5, label=\"Identity\")\n",
    "ax.hist(LJ2WCA_energy_transformed, bins=100, density=True, alpha=0.5, label=\"Transformed\")\n",
    "ax.set_xlabel(r\"$U(x)$\")\n",
    "ax.set_ylabel(r\"$P(U)$\")\n",
    "ax.set_title(\"Energy of Source System\")\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(os.path.join(output_dir, \"LJ2WCA_ener.png\"))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
